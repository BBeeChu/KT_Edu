{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, \\\n",
    "average_precision_score, roc_auc_score, plot_precision_recall_curve, plot_roc_curve, \\\n",
    "plot_confusion_matrix \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier, plot_importance\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.functions import test_model, get_timeseries_table, add_model,\\\n",
    "add_hypersearch\n",
    "\n",
    "from seaborn import heatmap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3590906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_score(optim_result):\n",
    "    \"\"\"\n",
    "    callback for a hyperparameter search.  Displays current best score and \n",
    "    best parameters.  To be added to the fit method, callback argument.\n",
    "    \"\"\"\n",
    "    score = opt.best_score_\n",
    "    params = pd.DataFrame(opt.best_params_, index=[0])\n",
    "    clear_output()\n",
    "    print('Best Average Precision Score So Far: ', score)\n",
    "    print('Using Parameters: ')\n",
    "    display(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953af53",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We start off by loading our data.  We only load data from the first half of the course because we want to identify students who may need intervention while there is still time for intervention to be effective.  The courses are 270 days long, so we load the first 135 days worth of data to use to make our predictions.  This date can be pushed forward by changing the `prediction_window` variable.  Later predictions are more accurate, earlier predictions can lead to more successful interventions.  It should be noted that these courses are self-paced, but assessments do have due dates.\n",
    "\n",
    "This data, after preprocessing the given trace data, contains several columns for each day of the course, up to the date we choose to make our prediction.  These metrics are:\n",
    "\n",
    "1. Number of activities completed on each day.  Activities vary from web links to quizzes.\n",
    "\n",
    "2. Number of clicks each day.\n",
    "\n",
    "3. A polynomial feature, clicks * activities, to capture the fact that different activities require different numbers of clicks.\n",
    "\n",
    "The last rows are not timeseries, but have to do with assessments and the course module that the data comes from.\n",
    "\n",
    "4. Assessment scores for each completed assessment.  I found in previous models that assessment scores are highly predictive of course outcomes. \n",
    "\n",
    "5. The timing of the assessment completion.  In previous exploration, there was a correlation between when students completed assessments and course outcomes.  Completing more assessments earlier in the course correlated to a greater chance of passing the course.\n",
    "\n",
    "6. One-hot-encoding of the course module code.  The course modules vary greatly in their pass/fail rates, average clicks and activities, and assessment scores.  Which course a student is taking is highly correlated to whether they will pass it. As you will see in the final model feature importances at the end of this notebook.\n",
    "\n",
    "## Class Balance\n",
    "\n",
    "The pass/fail ratio is about 2 to 1.  This is not such a terrible class balance, but my models consistently over-predict students to pass, and using the synthetic data oversampling technique called SMOTE helps to prevent the models from learning to just default to pass to improve their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1d93d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22875, 504)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_window = 135\n",
    "\n",
    "df = get_timeseries_table(prediction_window=prediction_window,\n",
    "                         binary_labels=True, one_hot_modules=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa93350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15364\n",
       "1     7511\n",
       "Name: final_result, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.final_result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86e90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['final_result'])\n",
    "y = df['final_result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=111)\n",
    "\n",
    "all_features = 'SMOTED activites, clicks, activities*clicks, assessments, modules'\n",
    "\n",
    "categoricals = [502, 501, 500, 499, 498, 497, 496]\n",
    "smotenc = SMOTENC(categoricals, random_state=111)\n",
    "X_train, y_train = smotenc.fit_resample(X_train, y_train)\n",
    "X_t, y_t = smotenc.fit_resample(X_t, y_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef57a7e",
   "metadata": {},
   "source": [
    "# Choosing from available models\n",
    "\n",
    "We will try several out of the box models on this data to see, in broad terms, which model might be most successful.  Later we will choose the most accurate ones and tune their parameters to make a final determination on which one to use.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "I chose ROC AUC as my target metric because we have an opportunity to tune the probability thresholds on the final model to find the right balance between recall and precision in the final classifier.  I want the model that performs best over all thresholds, so future stakeholders can make choices about how to balance unnecessary interventions and students needing intervention that don't get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7118fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(random_state=111, solver='liblinear'),\n",
    "         DecisionTreeClassifier(random_state=111),\n",
    "         RandomForestClassifier(random_state=111),\n",
    "         KNeighborsClassifier(),\n",
    "         SVC(random_state=111, probability=True),\n",
    "         SGDClassifier(loss='log', random_state=111),\n",
    "         AdaBoostClassifier(random_state=111),\n",
    "         XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=111),      \n",
    "         ]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_t, y_t)\n",
    "    add_model(model, X_t, y_t, X_val, y_val, \n",
    "              preprocessing=None, \n",
    "              features = all_features)\n",
    "\n",
    "    pd.read_csv('hyperparameter_table.csv').dropna(axis=0, subset=['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8d37e",
   "metadata": {},
   "source": [
    "## Model shortlist\n",
    "\n",
    "It turns out the tree based models are performing best on this data.  We will take the XGBoost, Random Forest, and Decision Tree models forward and see if we can tune the hyperparameters to achieve a better ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43dbea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Precision Score So Far:  0.941034382533646\n",
      "Using Parameters: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>objective</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.497976</td>\n",
       "      <td>logloss</td>\n",
       "      <td>0.094459</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.831388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colsample_bytree eval_metric  learning_rate  max_depth  min_child_weight  \\\n",
       "0          0.497976     logloss       0.094459         60                 1   \n",
       "\n",
       "         objective  scale_pos_weight  subsample  \n",
       "0  binary:logistic               2.0   0.831388  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb = XGBClassifier(use_label_encoder=False, seed=111, eval_metric='logloss')\n",
    "\n",
    "xgb_search_space = {'objective': Categorical(['binary:logistic',\n",
    "                                              'binary:hinge']),\n",
    "                    'eval_metric': Categorical(['logloss','error']),\n",
    "                   'learning_rate': Real(0.05, .3, 'log-uniform'),\n",
    "                   'min_child_weight': Integer(1,10, 'uniform'),\n",
    "                   'max_depth': Integer(20,60, 'normal'),\n",
    "                   'subsample': Real(0.3, 1, 'normal'),\n",
    "                   'colsample_bytree': Real(.3, 1.0, 'normal'),\n",
    "                   'scale_pos_weight': Real(.5, 2.0, 'uniform')}\n",
    "\n",
    "opt = BayesSearchCV(xgb, search_spaces=xgb_search_space, \n",
    "                    n_iter=50, cv=2,\n",
    "                    n_jobs=4,\n",
    "                    pre_dispatch = 8,\n",
    "                    random_state=111,\n",
    "                    return_train_score=True,\n",
    "                    scoring='average_precision')\n",
    "\n",
    "opt.fit(X_t, y_t, callback=[return_score])\n",
    "\n",
    "XGBmodel = opt.best_estimator_\n",
    "\n",
    "add_hypersearch(opt)\n",
    "\n",
    "add_model(XGBmodel, X_t, y_t, X_val, y_val, \n",
    "          features = all_features,\n",
    "          preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Precision Score So Far:  0.9380951595460184\n",
      "Using Parameters: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_weight</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_samples</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balanced</td>\n",
       "      <td>45</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>0.981449</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_weight  max_depth max_features  max_samples  n_estimators\n",
       "0     balanced         45         sqrt     0.981449           464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=111)\n",
    "\n",
    "rf_search_space = {'n_estimators': Integer(50, 500, 'normal'),\n",
    "                   'max_depth': Integer(5, 50, 'normal'),\n",
    "                   'max_features': Categorical(['sqrt','log2',None]),\n",
    "                   'class_weight': Categorical(['balanced','balanced_subsample',None]),\n",
    "                   'max_samples': Real(.1, .99, 'uniform')\n",
    "                   }\n",
    "\n",
    "opt = BayesSearchCV(rf, search_spaces=rf_search_space, \n",
    "                    n_iter=50, cv=2,\n",
    "                    n_jobs=4,\n",
    "                    pre_dispatch = 8,\n",
    "                    random_state=111,\n",
    "                    return_train_score=True,\n",
    "                    scoring='average_precision')\n",
    "\n",
    "opt.fit(X_t, y_t, callback=[return_score])\n",
    "\n",
    "RFclf = opt.best_estimator_\n",
    "\n",
    "add_hypersearch(opt)\n",
    "\n",
    "add_model(RFclf, X_t, y_t, X_val, y_val, \n",
    "          features=all_features, \n",
    "          preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615241a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(random_state=111)\n",
    "\n",
    "DT_search_space = {'criterion': Categorical(['gini','entropy']),\n",
    "                   'splitter': Categorical(['best','random']),\n",
    "                    'max_depth': Integer(3,10, 'uniform'),\n",
    "                   'max_features': Categorical(['sqrt','log2', None]),\n",
    "                   'class_weight': Categorical(['balanced',None]),\n",
    "                   }\n",
    "\n",
    "opt = BayesSearchCV(DT, search_spaces=DT_search_space, \n",
    "                    n_iter=50, cv=2,\n",
    "                    n_jobs=4,\n",
    "                    pre_dispatch = 8,\n",
    "                    random_state=111,\n",
    "                    scoring='average_precision',\n",
    "                    return_train_score=True)\n",
    "\n",
    "opt.fit(X_t, y_t, callback=[return_score])\n",
    "\n",
    "DTclf = opt.best_estimator_\n",
    "\n",
    "add_hypersearch(opt)\n",
    "\n",
    "add_model(DTclf, X_t, y_t, X_val, y_val, \n",
    "          features=all_features, \n",
    "          preprocessing=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a291792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = pd.read_csv('hyperparameter_table.csv').dropna(axis=0, subset=['val_accuracy'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c590f",
   "metadata": {},
   "source": [
    "# Final model:  XGBoost\n",
    "\n",
    "XGBoost, a tree based boosting ensemble model, has shown great success in Kaggle competitions, and has been a great model for previous projects.  I'm not surprised it provides the best results here.  Let's explore this model a little more. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "96a5bd3c",
=======
   "execution_count": 53,
   "id": "6580ff72",
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> parent of 5139e6a (follow-up to last)
=======
>>>>>>> parent of 5139e6a (follow-up to last)
=======
>>>>>>> parent of 5139e6a (follow-up to last)
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    clf=XGBmodel\n",
    "    print('Loading model from memory')\n",
    "except:\n",
    "    clf=pickle.load(open('time_series_xgb_best_smote.pkl', 'rb'))\n",
    "    print('Loading model from storage')\n",
    "    \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a26da",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix below is normalized across the true axis.  The top two boxes represent students who will truly pass the course.  The box in the top left is the percentage of passing students that the model accurately labeled as passing, and the to right is the percent of passing students labeled as likely not to pass, to fail or withdraw early.  Similarly the bottom left are students who will not pass, but the model predicts they will, and the bottom right are the percentage of students who will not pass that the model accurately predicts.\n",
    "\n",
    "We see here that the model still is over predicting student success.  26% of student in need of intervention will not receive it.  However, only 10% of students that model suggests for intervention will not need it.  I think there are just a lot of students who seem like they should pass, but don't.  Sometimes life happens, even to a strong student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5a2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_score = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "yhat = pd.Series(np.round_(y_score))\n",
    "\n",
    "confusion  = confusion_matrix(y_test, yhat, normalize='true')\n",
    "heatmap(confusion, annot=True, cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
    "plot_precision_recall_curve(clf, X_test, y_test, ax = axes[0])\n",
    "axes[0].set_title('Precision-Recall Curve')\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test, ax= axes[1])\n",
    "axes[1].set_title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b6a51",
   "metadata": {},
   "source": [
    "## Tuning the prediction probability threshold\n",
    "\n",
    "A stakeholder can tune the threshold probability that our model uses to classify a student as likely to pass or likely not to.  I use F1 score because it gives me an idea of both prevision and recall, where accuracy can be confounded by class imbalance in the test set.\n",
    "\n",
    "It turns out that a probability threshold of 41% gives us the best F1 score. But, you'll notice below that while it is quite accurate at predicting student who will pass, but much less so at predicting who needs interventions.  If interventions were very expensive, such as providing housing, regulary tutoring, childcare subsidies, etc., a stakeholder would want a high precision for the students assigned intervention, as shown below.  However, if interventions are cheap and failure is expensive they would want to adjust the probability threshold to improve the recall for students in danger of failing at the expense of more unneeded interventions for student who would succeed either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_accuracy = pd.DataFrame(columns= ['accuracy', 'f1_score'],\n",
    "                              index = [x/100 for x in range(60,80)])\n",
    "\n",
    "thresh_accuracy.index.name = 'Threshold'\n",
    "for thresh in range(10, 90):\n",
    "    thresh /= 100\n",
    "    yhat = pd.Series(y_score).apply(lambda x: 1 if x >= thresh else 0)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    f1 = f1_score(y_test, yhat)\n",
    "    roc = roc_auc_score(y_test, y_score)\n",
    "    pr = average_precision_score(y_test, y_score)\n",
    "    thresh_accuracy.loc[thresh, 'accuracy'] = accuracy\n",
    "    thresh_accuracy.loc[thresh, 'f1_score'] = f1\n",
    "\n",
    "print('ROC_AUC: ', roc)\n",
    "print('PR_AUC: ', pr)\n",
    "thresh_accuracy = thresh_accuracy.sort_values(by='f1_score', ascending=False)\n",
    "thresh_accuracy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b71e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_thresh = thresh_accuracy.index[0]\n",
    "\n",
    "yhat = pd.Series(y_score).apply(lambda x: 1 if x >= best_thresh else 0)\n",
    "\n",
    "confusion  = confusion_matrix(y_test, yhat, normalize='true')\n",
    "heatmap(confusion, annot=True, cmap='Greens',\n",
    "       xticklabels = ['Predict Pass', 'Predict Fail'],\n",
    "       yticklabels = ['True Pass', 'True Fail'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11783a2",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "The below chart ranks the importance of each feature, according to the XGBoost model, from most important leas important.  It seems that whether or not the student is taking the module coded as `GGG` is a big predictor!  It may be that this is the hardest, or easiest of the modules.  We can explore this more later.  We also see that other modules rank high on the list.  It seems that which course a student is in plays an outsized role in determining success.  This might be worth the university's time to look into.  Are some students in courses they aren't prepared for?  Are some instructors going to easy on students in some courses?\n",
    "\n",
    "The next few features of greatest importance are assessment scores and activities completed relatively close to the prediction window cut-off of 135 days.\n",
    "\n",
    "From these results it seems that our model is relying on assessments scores and work done in the middle in the course to predict success.  It is possible that work done near the end is more predictive, but that information is not available to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cfbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(clf.feature_importances_, index=X_test.columns)\n",
    "importance.sort_values(by=0, ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
   "id": "4d31420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('time_series_xgb_best_smote.pkl','wb'))"
=======
=======
>>>>>>> parent of 5139e6a (follow-up to last)
=======
>>>>>>> parent of 5139e6a (follow-up to last)
   "execution_count": 54,
   "id": "8fcda229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('time_series_xgb_best.pkl','wb'))"
<<<<<<< HEAD
<<<<<<< HEAD
>>>>>>> parent of 5139e6a (follow-up to last)
=======
>>>>>>> parent of 5139e6a (follow-up to last)
=======
>>>>>>> parent of 5139e6a (follow-up to last)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6871471",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
    "XGBoost was our most successfuly classifier for recommending students for intervention.  It successfully recommends interventions for 75% of students that need them, while recommending interventions for 15% of students who don't need them.  These numbers could bear improvement, but show proof of concept that student/virtual learning environment interactions can be used to predict student success.\n",
=======
    "XGBoost was our most successfuly classifier for recommending interventions for students.  It successfully recommends interventions for 74% of students that need them, while recommending interventions for 10% of students who don't need them.  These numbers could bear improvement, but show proof of concept that student/virtual learning environment interactions can be used to predict student success.\n",
>>>>>>> parent of 5139e6a (follow-up to last)
=======
    "XGBoost was our most successfuly classifier for recommending interventions for students.  It successfully recommends interventions for 74% of students that need them, while recommending interventions for 10% of students who don't need them.  These numbers could bear improvement, but show proof of concept that student/virtual learning environment interactions can be used to predict student success.\n",
>>>>>>> parent of 5139e6a (follow-up to last)
=======
    "XGBoost was our most successfuly classifier for recommending interventions for students.  It successfully recommends interventions for 74% of students that need them, while recommending interventions for 10% of students who don't need them.  These numbers could bear improvement, but show proof of concept that student/virtual learning environment interactions can be used to predict student success.\n",
>>>>>>> parent of 5139e6a (follow-up to last)
    "\n",
    "It's also important to keep in mind which course a student is enrolled in.  It should not be surprising that some college courses are harder than others.  That's kind of common sense.\n",
    "\n",
    "# Future research\n",
    "\n",
    "Future researchers for this dataset might try correlating the types of activities students engage each day with student success.  Some information on this is available in the raw dataset.\n",
    "\n",
    "They also might look more closely at why early, but not the first, assessment scores are so important.  It may be that reaching out to students who perform poorly on the 2nd and 3rd assessments would be a quick way to target interventions.  Maybe most students are energized to do well on the first assessment, or that first assessments tend to be easier.  There is room for study here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea05e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('hyperparameter_table.csv')\n",
    "table = table[table['model'] == 'XGBClassifier'].dropna(axis=1, how='all')\n",
    "table.to_csv('XGB_hyperparameter_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889ee4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student_predictor_env",
   "language": "python",
   "name": "student_predictor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
